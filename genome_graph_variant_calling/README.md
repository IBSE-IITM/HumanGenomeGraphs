<h1>Genome Graph based variant calling</h1>

This repo contains codes to construct genome graphs and analyse whole genome sequences with them. <br>
Kindly execute the steps linearly and conserve the directory format inside gg_workflow subdirectory. <br>
*Tested on Linux/Unix machines*

<br>

**Creating and activating the conda environnment**

The tools required for performing genome graph related analyses can be installed using conda [Install Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html). <br>
To install all the tools in one go, create a conda environment using the yml file provied with this repo at ```gg_workflow/envs/gg_environment.yaml```.

1) Create the conda virtual environment: 
```
conda env create -f gg_environment.yaml
```

To verify if above step ran smoothly, run ```conda info --envs``` and check if ```gg_workflow``` gets listed. 

2) Activate the environment created: 
```
conda activate gg_workflow
```

<br>

**Genome graph construction**

Follow the steps in ```genome_graph_construction.md``` file [link](https://github.com/GenomeIndia/iitm/blob/main/genome_graph_construction.md).

> Kindly note that the steps listed as *VCF procesing* needs to be done on a case-by-case basis depending on the input VCF file. When using the VCF file generated by the containerized BWA-GATK pipeline shared by [IITM via dockerhub](https://hub.docker.com/r/ibse/genome-india), all the steps are recommended.



<br> 

**Analysis of WGS w.r.t. genome graphs**

The ```gg_workflow``` subdirectory contains a portable Snakemake based workflow that can be used to analyse WGS with respect to the genome graphs created in the previous section.

1) Move the quality check passed FASTQ (input WGS) files to ```gg_workflow/data/samples```

2) Edit the filepaths of the genome graph indices in ```gg_workflow/config/config.yaml```. These filepaths correspond to the outputs created when running the genome graph construction commands.

3) Activate the ```gg_workflow``` conda environment.
```
conda activate gg_workflow
```

4) Perform a Snakemake dry run to ensure that the workflow works.
```
snakemake -npr
``` 


5) Once the dry run has run successfully, the analysis can be started with the command below:
```
snakemake \ # Invoke Snakemake
--jobs 5 \ # Specify no. of parallel jobs to be run
-p -r --use-conda \ 
--cluster-config config/cluster.json \ # To be added only for HPCs
--cluster "qsub -S /bin/bash -l nodes={cluster.nodes}:ppn={cluster.ppn} # To be added only for HPCs
```

> Note: Edit the parameters accordingly <br>
> --jobs : max number of jobs to be performed parallely. <br>
> --use-conda: to enable conda through job scheduler <br>
>
> *Optional parameters:* <br>
> -p & -r: print the command and the reason to run the command
> --cluster-config: points to a file that specifies resource allocation. This file can be edited to as per the resource available. At present, 25 threads are sent to each process by default. Kindly change this as per the resources available. <br> 
> --cluster: the qsub command that snakemake uses to submit individual jobs. <br>
> --forceall: Mandatorily run all the rules in the Snakefile
> --rerun-incomplete: If the analysis stops before completion, this parameter can be added to recreate any incomplete files.


Kindly look into [Snakemake documentation](https://snakemake.readthedocs.io/en/stable/) for more details.